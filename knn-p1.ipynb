{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c408eb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de los datos:\n",
      "Número de muestras: 7352\n",
      "Pasos de tiempo: 128\n",
      "Número de etiquetas: 7352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10000/10000 [00:02<00:00, 3878.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesado chunk 1 de 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10000/10000 [00:02<00:00, 3731.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesado chunk 2 de 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10000/10000 [00:02<00:00, 3524.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesado chunk 3 de 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10000/10000 [00:04<00:00, 2272.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesado chunk 4 de 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10000/10000 [00:04<00:00, 2261.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesado chunk 5 de 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10000/10000 [00:03<00:00, 2745.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesado chunk 6 de 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10000/10000 [00:04<00:00, 2420.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesado chunk 7 de 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 3520/3520 [00:01<00:00, 2115.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesado chunk 8 de 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 9000/9000 [00:04<00:00, 2184.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesado chunk 1 de 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 9000/9000 [00:03<00:00, 2651.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesado chunk 2 de 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 8523/8523 [00:03<00:00, 2636.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesado chunk 3 de 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "import h5py\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def cargar_datos(path, is_train=True):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        body_acc_x = f['body_acc_x'][:]\n",
    "        body_acc_y = f['body_acc_y'][:]\n",
    "        body_acc_z = f['body_acc_z'][:]\n",
    "        body_gyro_x = f['body_gyro_x'][:]\n",
    "        body_gyro_y = f['body_gyro_y'][:]\n",
    "        body_gyro_z = f['body_gyro_z'][:]\n",
    "        total_acc_x = f['total_acc_x'][:]\n",
    "        total_acc_y = f['total_acc_y'][:]\n",
    "        total_acc_z = f['total_acc_z'][:]\n",
    "        \n",
    "        if is_train:\n",
    "            labels = f['y'][:]\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "    time_steps = body_acc_x.shape[1]\n",
    "    \n",
    "    if is_train:\n",
    "        print(\"Dimensiones de los datos:\")\n",
    "        print(f\"Número de muestras: {body_acc_x.shape[0]}\")\n",
    "        print(f\"Pasos de tiempo: {time_steps}\")\n",
    "        print(f\"Número de etiquetas: {labels.shape[0]}\")\n",
    "        \n",
    "        labels_repeated = np.repeat(labels, time_steps)\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'body_acc_x': body_acc_x.flatten(),\n",
    "        'body_acc_y': body_acc_y.flatten(),\n",
    "        'body_acc_z': body_acc_z.flatten(),\n",
    "        'body_gyro_x': body_gyro_x.flatten(),\n",
    "        'body_gyro_y': body_gyro_y.flatten(),\n",
    "        'body_gyro_z': body_gyro_z.flatten(),\n",
    "        'total_acc_x': total_acc_x.flatten(),\n",
    "        'total_acc_y': total_acc_y.flatten(),\n",
    "        'total_acc_z': total_acc_z.flatten()\n",
    "    })\n",
    "    \n",
    "    if is_train:\n",
    "        data['label'] = labels_repeated\n",
    "    \n",
    "    data['time'] = np.tile(np.arange(time_steps), data.shape[0] // time_steps)\n",
    "    data['id'] = np.repeat(np.arange(data.shape[0] // time_steps), time_steps)\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_data = cargar_datos('train.h5', is_train=True)\n",
    "\n",
    "test_data = cargar_datos('test.h5', is_train=False)\n",
    "fc_parameters = MinimalFCParameters()\n",
    "\n",
    "def extract_features_in_chunks(data, chunk_size=1000, n_jobs=1):\n",
    "    all_features = []\n",
    "    for i in range(0, data['id'].nunique(), chunk_size):\n",
    "        chunk_ids = range(i, min(i + chunk_size, data['id'].nunique()))\n",
    "        chunk_data = data[data['id'].isin(chunk_ids)]\n",
    "        \n",
    "        features_chunk = extract_features(\n",
    "            chunk_data,\n",
    "            column_id='id',\n",
    "            column_sort='time',\n",
    "            default_fc_parameters=fc_parameters,\n",
    "            n_jobs=n_jobs \n",
    "        )\n",
    "        all_features.append(features_chunk)\n",
    "        print(f\"procesado chunk {i//chunk_size + 1} de {(data['id'].nunique() + chunk_size - 1)//chunk_size}\")\n",
    "    \n",
    "    return pd.concat(all_features)\n",
    "\n",
    "features_train = extract_features_in_chunks(train_data, n_jobs=1)\n",
    "features_test = extract_features_in_chunks(test_data, n_jobs=1)\n",
    "if 'label' in train_data.columns:\n",
    "    labels_train = train_data['label'].iloc[::128].reset_index(drop=True)\n",
    "    labels_train.to_pickle('labels_train.pkl')\n",
    "\n",
    "features_train.to_pickle('features_train.pkl')\n",
    "features_test.to_pickle('features_test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e8115bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "features_train = pd.read_pickle('features_train.pkl')\n",
    "features_test = pd.read_pickle('features_test.pkl')\n",
    "labels_train = pd.read_pickle('labels_train.pkl')\n",
    "\n",
    "features_test = features_test.replace([np.inf, -np.inf], np.nan)\n",
    "features_train = features_train.fillna(0)  \n",
    "features_test = features_test.fillna(0)\n",
    "\n",
    "features_test = features_test.reindex(columns=features_train.columns, fill_value=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(features_train)\n",
    "X_test_scaled = scaler.transform(features_test)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, labels_train)\n",
    "\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights='distance',\n",
    "    metric='euclidean',\n",
    "    n_jobs=-1\n",
    ")\n",
    "knn.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "joblib.dump(knn, 'KNN_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "np.save('X_test_scaled.npy', X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f4431f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "knn_model = joblib.load('KNN_model.pkl')\n",
    "X_test_scaled = np.load('X_test_scaled.npy')\n",
    "\n",
    "test_predictions = knn_model.predict(X_test_scaled)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(1, len(test_predictions) + 1),\n",
    "    'activity': test_predictions\n",
    "})\n",
    "\n",
    "submission['activity'] = submission['activity'].astype(int)\n",
    "\n",
    "submission_file = 'prediccionknn.csv'\n",
    "submission.to_csv(submission_file, index=False, header=['ID', 'Activity'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
